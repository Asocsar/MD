{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import random as rand\n",
    "import numpy as np\n",
    "import math\n",
    "import sklearn.naive_bayes as nB\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('games_clean.csv')\n",
    "random = 24\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in data.columns[1:]:\n",
    "    print(\"Before\", name + '\\n', data[data[name].isna()][name])\n",
    "    data[name].fillna(data[name].mean(), inplace=True)\n",
    "    print(\"After\", name + '\\n', data[data[name].isna()][name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Metacritic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = data['Metacritic'].unique()\n",
    "Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "\n",
    "exponential = ['Presence', 'OriginalCost', 'Achievements', 'Storage', 'RatingsBreakdown-Recommended', 'RatingsBreakdown-Meh', 'RatingsBreakdown-Exceptional', 'RatingsBreakdown-Skip']\n",
    "lambdas = {}\n",
    "\n",
    "for name in exponential:\n",
    "    boc = boxcox(data.loc[:,name].apply(lambda x: x + 1*10**(-10)))\n",
    "    data.loc[:,name] = boc[0]\n",
    "    print(name, boc[1])\n",
    "    lambdas[name] = boc[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for name in ['Presence', 'Memory', 'ReleaseDate', 'OriginalCost', 'DiscountedCost', 'Achievements', 'Storage', 'RatingsBreakdown-Recommended', 'RatingsBreakdown-Meh', 'RatingsBreakdown-Exceptional', 'RatingsBreakdown-Skip']:\n",
    "    data.loc[:,name] = data[name].apply(lambda x: np.log2(x+0.000001))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_analysis = data[['Presence',\n",
    "'Memory',\n",
    "'ReleaseDate',\n",
    "'OriginalCost',\n",
    "'DiscountedCost',\n",
    "'Achievements',\n",
    "'Storage',\n",
    "'RatingsBreakdown-Recommended',\n",
    "'RatingsBreakdown-Meh',\n",
    "'RatingsBreakdown-Exceptional',\n",
    "'RatingsBreakdown-Skip']]\n",
    "\n",
    "\n",
    "\n",
    "hist = data_analysis.hist(bins=50, figsize=(20,20))\n",
    "box = data_analysis.boxplot()\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.xticks(rotation='horizontal')\n",
    "for name in data_analysis.columns:\n",
    "    fig, axes= plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 4]}, figsize=(9,5))\n",
    "    data_analysis.boxplot(column=name,ax=axes[0])\n",
    "    data_analysis.hist(column=name, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "for name in data.columns:\n",
    "    print('\\033[1m'+name+'\\033[0;0m')\n",
    "    print(data[name].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "Labels_encoded = encoder.fit_transform(Labels)\n",
    "Labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_expected = encoder.fit_transform(data['Metacritic'])\n",
    "data_input = data.loc[:, data.columns != 'Metacritic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_input, data_expected, test_size=0.3,random_state=404)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "bnb = BernoulliNB(alpha=0.2)\n",
    "mnb = MultinomialNB(alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Metacritic'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(encoder.inverse_transform(y_pred)).value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.reshape(3000,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.reshape(3000,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[:5], y_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracy_split = []\n",
    "for i in range(20,100):\n",
    "    print('')\n",
    "    print('*-----------* KFOLD', i, '*-----------*')\n",
    "    cv = KFold(n_splits=i, shuffle=False)\n",
    "    cv_scores = cross_validate(bnb, X=X_train, y=y_train, cv=cv, scoring='accuracy', return_estimator=True)\n",
    "    print('Max Accuracy in train', '--->', max([(sc,i) for i,sc in enumerate(cv_scores['test_score'])])[0])\n",
    "    print('Mean Accuracy in train', '--->', np.mean(cv_scores['test_score']))\n",
    "    all_accuracy_split.append(np.mean(cv_scores['test_score']))\n",
    "    bnb = cv_scores['estimator'][max([(sc,i) for i,sc in enumerate(cv_scores['test_score'])])[1]]\n",
    "    print('Max Accuracy in test', '--->', accuracy_score(y_test,bnb.predict(X_test)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracy = []\n",
    "for i in range(20,100):\n",
    "    print('')\n",
    "    print('*-----------* KFOLD', i, '*-----------*')\n",
    "    cv = KFold(n_splits=i, shuffle=False)\n",
    "    cv_scores = cross_validate(bnb, X=data_input, y=data_expected, cv=cv, scoring='accuracy', return_estimator=True)\n",
    "    print('Max Accuracy in train', '--->', max([(sc,i) for i,sc in enumerate(cv_scores['test_score'])])[0])\n",
    "    print('Mean Accuracy in train', '--->', np.mean(cv_scores['test_score']))\n",
    "    all_accuracy.append(np.mean(cv_scores['test_score']))\n",
    "    bnb = cv_scores['estimator'][max([(sc,i) for i,sc in enumerate(cv_scores['test_score'])])[1]]\n",
    "    print('Max Accuracy in test', '--->', accuracy_score(data_expected,bnb.predict(data_input)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(20,100)), all_accuracy_split, label = \"cross-validation + hold out\")\n",
    "plt.plot(list(range(20,100)), all_accuracy, label = \"cross-validation k > 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}